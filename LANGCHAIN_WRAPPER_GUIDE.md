# üîó Gu√≠a del Wrapper LangChain para Gemini

## üìã Descripci√≥n

Esta implementaci√≥n a√±ade un wrapper opcional de LangChain para el cliente de Google Gemini, proporcionando capacidades avanzadas de debugging, monitoreo y an√°lisis sin modificar el c√≥digo existente.

## ‚ú® Caracter√≠sticas

### üéØ Funcionalidades A√±adidas

1. **Callbacks Integrados** - Monitoreo de llamadas al LLM
2. **Streaming de Respuestas** - Respuestas en tiempo real
3. **Token Counting** - Conteo autom√°tico de tokens
4. **LangSmith Integration** - Debugging avanzado (opcional)
5. **Compatibilidad Total** - Funciona con el c√≥digo existente sin cambios

## üöÄ Instalaci√≥n

### 1. Instalar Dependencia

```bash
pip install langchain-google-genai
```

O actualizar todas las dependencias:

```bash
pip install -r requirements.txt
```

### 2. Habilitar el Wrapper

Edita `src/.env` y a√±ade:

```env
USE_LANGCHAIN_WRAPPER=true
```

## üìä Uso

### Modo B√°sico (Sin Cambios en el C√≥digo)

El wrapper se activa autom√°ticamente cuando `USE_LANGCHAIN_WRAPPER=true`:

```python
from llm.gemini_client import call_gemini

# Esta llamada usar√° el wrapper de LangChain autom√°ticamente
response = call_gemini(role_prompt, context)
```

**Nota:** El wrapper solo se usa para llamadas simples (sin `response_schema` ni `allow_use_tool`).

### Modo Avanzado (Uso Directo)

Para usar caracter√≠sticas avanzadas, importa directamente:

```python
from llm.langchain_gemini import create_langchain_llm, call_gemini_with_langchain

# Crear instancia del LLM
llm = create_langchain_llm(streaming=True)

# Usar con streaming
response = call_gemini_with_langchain(
    role_prompt="Eres un asistente √∫til",
    context="Explica qu√© es LangChain",
    streaming=True
)
```

### Token Counting

```python
from llm.langchain_gemini import get_token_count

text = "Este es un texto de ejemplo para contar tokens."
token_info = get_token_count(text)

print(f"Tokens: {token_info['total_tokens']}")
print(f"Modelo: {token_info['model']}")
```

### Callbacks Personalizados

```python
from langchain_core.callbacks import BaseCallbackHandler
from llm.langchain_gemini import create_langchain_llm

class MyCustomCallback(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"üöÄ Iniciando llamada al LLM...")
    
    def on_llm_end(self, response, **kwargs):
        print(f"‚úÖ Llamada completada")

# Usar con callbacks personalizados
llm = create_langchain_llm(callbacks=[MyCustomCallback()])
```

## üß™ Testing

Ejecuta el script de prueba:

```bash
python test_langchain_wrapper.py
```

Este script verifica:
- ‚úÖ Importaciones correctas
- ‚úÖ Configuraci√≥n v√°lida
- ‚úÖ Funcionamiento del wrapper
- ‚úÖ Compatibilidad con c√≥digo existente

## üîÑ Modos de Operaci√≥n

### 1. Cliente Directo (Por Defecto)

```env
USE_LANGCHAIN_WRAPPER=false
```

- Usa `google.genai.Client` directamente
- Menor overhead
- Ideal para producci√≥n

### 2. Wrapper LangChain (Opcional)

```env
USE_LANGCHAIN_WRAPPER=true
```

- Usa `ChatGoogleGenerativeAI` de LangChain
- Callbacks y streaming
- Ideal para desarrollo y debugging

### 3. Modo Mock (Testing)

```env
LLM_MOCK_MODE=true
```

- No hace llamadas reales al LLM
- Usa respuestas simuladas
- Ideal para testing sin API key

## üìà Ventajas del Wrapper

### Para Desarrollo

- **Debugging Mejorado**: Callbacks para rastrear cada llamada
- **Monitoreo**: M√©tricas autom√°ticas de uso
- **Streaming**: Ver respuestas en tiempo real
- **Token Tracking**: Optimizar costos

### Para Producci√≥n

- **Compatibilidad**: Funciona con c√≥digo existente
- **Fallback Autom√°tico**: Si falla, usa cliente directo
- **Configuraci√≥n Flexible**: Activar/desactivar sin cambios de c√≥digo

## üéØ Casos de Uso

### 1. Debugging de Prompts

```python
from langchain_core.callbacks import StdOutCallbackHandler
from llm.langchain_gemini import create_langchain_llm

# Ver todas las llamadas en consola
llm = create_langchain_llm(
    streaming=True,
    callbacks=[StdOutCallbackHandler()]
)
```

### 2. An√°lisis de Costos

```python
from llm.langchain_gemini import get_token_count

# Estimar tokens antes de llamar
prompt = "Tu prompt muy largo aqu√≠..."
tokens = get_token_count(prompt)

if tokens['total_tokens'] > 1000:
    print("‚ö†Ô∏è Prompt muy largo, considera resumir")
```

### 3. Integraci√≥n con LangSmith

```bash
# Configurar LangSmith (opcional)
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=tu-api-key
export LANGCHAIN_PROJECT=capstone-multiagente
```

Todas las llamadas se registrar√°n autom√°ticamente en LangSmith para an√°lisis.

## üîß Configuraci√≥n Avanzada

### Variables de Entorno

```env
# Wrapper LangChain
USE_LANGCHAIN_WRAPPER=true

# LangSmith (opcional - para debugging avanzado)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=tu-langsmith-api-key
LANGCHAIN_PROJECT=mi-proyecto

# Configuraci√≥n del modelo (heredada)
MODEL_NAME=gemini-2.5-flash
TEMPERATURE=0.1
MAX_OUTPUT_TOKENS=4000
```

## üêõ Troubleshooting

### Error: "No module named 'langchain_google_genai'"

```bash
pip install langchain-google-genai
```

### El wrapper no se activa

1. Verifica que `USE_LANGCHAIN_WRAPPER=true` en `src/.env`
2. Revisa los logs: debe aparecer "‚úÖ Wrapper de LangChain habilitado"
3. Aseg√∫rate de que no est√©s usando `response_schema` o `allow_use_tool`

### Fallback al cliente directo

El wrapper autom√°ticamente usa el cliente directo si:
- Hay un error en la importaci√≥n
- La llamada usa `response_schema` (Product Owner)
- La llamada usa `allow_use_tool` (herramientas)

Esto es intencional para mantener compatibilidad.

## üìö Referencias

- [LangChain Documentation](https://python.langchain.com/)
- [ChatGoogleGenerativeAI](https://python.langchain.com/docs/integrations/chat/google_generative_ai)
- [LangSmith](https://docs.smith.langchain.com/)
- [Google Gemini API](https://ai.google.dev/docs)

## üéâ Resumen

El wrapper de LangChain es **opcional** y proporciona:

‚úÖ **Debugging avanzado** sin modificar c√≥digo
‚úÖ **Monitoreo de tokens** para optimizar costos
‚úÖ **Streaming** para mejor UX
‚úÖ **Compatibilidad total** con el sistema existente
‚úÖ **Fallback autom√°tico** si hay problemas

**Recomendaci√≥n:**
- **Desarrollo**: `USE_LANGCHAIN_WRAPPER=true` para debugging
- **Producci√≥n**: `USE_LANGCHAIN_WRAPPER=false` para performance
