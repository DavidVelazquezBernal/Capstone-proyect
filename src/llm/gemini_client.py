"""
Cliente LLM para interacci√≥n con Google Gemini.
Incluye manejo de errores y reintentos autom√°ticos.
Soporta wrapper de LangChain opcional para debugging avanzado.
"""

import os
import time
from typing import Optional
from pydantic import BaseModel
from google import genai
from google.genai.errors import APIError
from config.settings import settings
from utils.logger import setup_logger
from tools.code_executor import CodeExecutionToolWithInterpreterPY, CodeExecutionToolWithInterpreterTS
from llm.mock_responses import get_mock_response

logger = setup_logger(__name__, level=settings.get_log_level())

# Importaci√≥n condicional del wrapper de LangChain
_langchain_available = False
if settings.USE_LANGCHAIN_WRAPPER:
    try:
        from llm.langchain_gemini import call_gemini_with_langchain, get_token_count
        _langchain_available = True
        logger.info("‚úÖ Wrapper de LangChain habilitado")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è No se pudo importar wrapper de LangChain: {e}")
        logger.warning("   Instala: pip install langchain-google-genai")
        _langchain_available = False

# Inicializaci√≥n del cliente Gemini
# try:
#     # Intentar usar userdata de Colab
#     from google.colab import userdata
#     os.environ["GEMINI_API_KEY"] = userdata.get('gen-lang-client-0440601098')
#     client = genai.Client()
#     print("‚úÖ Cliente Gemini inicializado correctamente (Colab).")
# except ImportError:
    # Entorno local - usar .env
if settings.GEMINI_API_KEY:
    client = genai.Client(api_key=settings.GEMINI_API_KEY)
    logger.info("‚úÖ Cliente Gemini inicializado correctamente (Local).")
else:
    logger.warning("‚ö†Ô∏è WARNING: GEMINI_API_KEY no configurada. El cliente puede fallar.")
    client = None
# except Exception as e:
#     print(f"‚ùå ERROR: Fallo al inicializar el cliente Gemini. {e}")
#     client = None


def call_gemini(
    role_prompt: str, 
    context: str, 
    response_schema: Optional[BaseModel] = None, 
    allow_use_tool: bool = False
) -> str:
    """
    Realiza una llamada a Gemini 2.5 Flash con el prompt de rol y el contexto.
    
    Args:
        role_prompt (str): El prompt que define el rol y las instrucciones del agente
        context (str): El contexto actual del proyecto
        response_schema (BaseModel, optional): Schema Pydantic para validaci√≥n de respuesta JSON
        allow_use_tool (bool): Si se permite el uso de herramientas (tools)
    
    Returns:
        str: La respuesta del modelo LLM
    """
    # MODO MOCK - Evitar llamadas reales al LLM durante testing
    if settings.LLM_MOCK_MODE:
        logger.info("üß™ [MOCK] Devolviendo respuesta mockeada (LLM_MOCK_MODE=true)")
        return get_mock_response(role_prompt, context)
    
    # MODO LANGCHAIN - Usar wrapper de LangChain si est√° habilitado
    # Nota: Solo para llamadas simples sin response_schema ni tools
    if settings.USE_LANGCHAIN_WRAPPER and _langchain_available:
        if response_schema is None and not allow_use_tool:
            logger.debug("üîó Usando wrapper de LangChain")
            try:
                return call_gemini_with_langchain(role_prompt, context)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error con wrapper LangChain, fallback a cliente directo: {e}")
                # Continuar con el cliente directo si falla
    
    if not client:
        return "ERROR: Cliente Gemini no inicializado correctamente."

    full_prompt = (
        f"{role_prompt}\n\n"
        f"--- DATOS ACTUALES DEL PROYECTO ---\n"
        f"{context}\n\n"
        f"--- TAREA ---\n"
    )

    config = {
        "temperature": settings.TEMPERATURE,
        "max_output_tokens": settings.MAX_OUTPUT_TOKENS
    }

    if response_schema:
        # Rama del Product Owner - salida JSON estructurada
        config["response_mime_type"] = "application/json"
        config["response_schema"] = response_schema.model_json_schema()
        full_prompt += (
            f"GENERA EL OUTPUT √öNICAMENTE EN FORMATO JSON que se adhiera al siguiente "
            f"esquema Pydantic: {response_schema.__name__}. "
            f"No a√±adas explicaciones ni texto adicional."
        )
    elif allow_use_tool:
        
        # Proveer ambas herramientas para que el modelo elija seg√∫n el lenguaje
        available_tools = [CodeExecutionToolWithInterpreterPY, CodeExecutionToolWithInterpreterTS]
        config["tools"] = available_tools
        full_prompt += "Genera √∫nicamente el bloque de texto solicitado en tu Output Esperado. No a√±adas explicaciones."
    else:
        full_prompt += "Genera √∫nicamente el bloque de texto solicitado en tu Output Esperado. No a√±adas explicaciones."

    try:
        response = client.models.generate_content(
            model=settings.MODEL_NAME,
            contents=full_prompt,
            config=config,
        )
        if not response.text or response.text == "None" or response.text.lower() == "none":
            logger.error(f"\n{'='*60}")
            logger.error("‚ùå ERROR: EL LLM NO DEVOLVI√ì RESPUESTA V√ÅLIDA")
            logger.error(f"{'='*60}")
            logger.error(f"üìã Informaci√≥n de diagn√≥stico:")
            logger.error(f"   ‚Ä¢ Modelo usado: {settings.MODEL_NAME}")
            logger.error(f"   ‚Ä¢ Respuesta vac√≠a: {response.text is None or response.text == ''}")
            logger.error(f"   ‚Ä¢ Valor de response.text: {repr(response.text)}")
            logger.error(f"   ‚Ä¢ Tipo de response: {type(response)}")
            
            # Verificar si hay candidatos en la respuesta
            if hasattr(response, 'candidates') and response.candidates:
                logger.error(f"   ‚Ä¢ Candidatos disponibles: {len(response.candidates)}")
                for i, candidate in enumerate(response.candidates):
                    logger.error(f"   ‚Ä¢ Candidato {i+1}:")
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason = str(candidate.finish_reason)
                        logger.error(f"     - Finish reason: {finish_reason}")
                        
                        # Diagn√≥stico espec√≠fico para MALFORMED_FUNCTION_CALL
                        if "MALFORMED_FUNCTION_CALL" in finish_reason:
                            logger.error(f"\n{'='*60}")
                            logger.error("üîß DIAGN√ìSTICO: MALFORMED_FUNCTION_CALL")
                            logger.error(f"{'='*60}")
                            logger.error(f"El modelo intent√≥ llamar a una herramienta pero la llamada est√° mal formada.")
                            logger.error(f"\nüìä Detalles del candidato:")
                            
                            # Mostrar contenido completo del candidato
                            if hasattr(candidate, 'content') and candidate.content:
                                logger.error(f"   ‚Ä¢ Contenido del candidato:")
                                logger.error(f"     {candidate.content}")
                                
                                # Verificar si hay function_calls
                                if hasattr(candidate.content, 'parts'):
                                    logger.error(f"\n   ‚Ä¢ Partes del contenido ({len(candidate.content.parts)} partes):")
                                    for j, part in enumerate(candidate.content.parts):
                                        logger.error(f"     - Parte {j+1}: {type(part).__name__}")
                                        if hasattr(part, 'function_call'):
                                            logger.error(f"       ‚Üí Function call detectada:")
                                            logger.error(f"         Nombre: {part.function_call.name if hasattr(part.function_call, 'name') else 'N/A'}")
                                            logger.error(f"         Argumentos: {part.function_call.args if hasattr(part.function_call, 'args') else 'N/A'}")
                                        elif hasattr(part, 'text'):
                                            logger.error(f"       ‚Üí Texto: {part.text[:200]}...")
                            
                            # Mostrar herramientas disponibles
                            if allow_use_tool:
                                logger.error(f"\n   ‚Ä¢ Herramientas configuradas:")
                                if 'tools' in config:
                                    for tool in config['tools']:
                                        tool_name = tool.__name__ if hasattr(tool, '__name__') else str(tool)
                                        logger.error(f"     - {tool_name}")
                            
                            logger.error(f"\nüí° Posibles causas:")
                            logger.error(f"   1. El modelo gener√≥ argumentos con formato JSON inv√°lido")
                            logger.error(f"   2. Los argumentos no coinciden con el schema de la herramienta")
                            logger.error(f"   3. Falta alg√∫n argumento requerido por la herramienta")
                            logger.error(f"   4. El nombre de la funci√≥n es incorrecto")
                            logger.error(f"{'='*60}\n")
                    
                    if hasattr(candidate, 'safety_ratings'):
                        logger.error(f"     - Safety ratings: {candidate.safety_ratings}")
                    if hasattr(candidate, 'content'):
                        logger.error(f"     - Content disponible: {candidate.content is not None}")
            else:
                logger.error(f"   ‚Ä¢ No hay candidatos en la respuesta")
            
            # Verificar bloqueos de seguridad
            if hasattr(response, 'prompt_feedback'):
                logger.error(f"   ‚Ä¢ Prompt feedback: {response.prompt_feedback}")
            
            logger.error(f"{'='*60}\n")
            raise APIError("El LLM devolvi√≥ None o respuesta vac√≠a.")
        return response.text

    except APIError as e:
        # Detectar errores 503 (Service Unavailable) o sobrecarga
        error_message = str(e)
        
        if "503" in error_message or "UNAVAILABLE" in error_message or "overloaded" in error_message.lower():
            logger.error(f"\n{'='*60}")
            logger.error("‚ö†Ô∏è ERROR 503: SERVICIO SOBRECARGADO")
            logger.error(f"{'='*60}")
            logger.error(f"‚ùå El modelo de Gemini est√° sobrecargado")
            logger.error(f"üìä Detalles: {e}")
            logger.error(f"\nüîÑ REINTENTANDO con espera exponencial...")
            logger.error(f"{'='*60}\n")
            
            # Reintentar con backoff exponencial
            max_retries = settings.MAX_API_RETRIES
            for attempt in range(1, max_retries + 1):
                wait_time = settings.RETRY_BASE_DELAY ** attempt  # 2, 4, 8 segundos
                logger.warning(f"üîÑ Intento {attempt}/{max_retries} - Esperando {wait_time}s...")
                time.sleep(wait_time)
                
                try:
                    response = client.models.generate_content(
                        model=settings.MODEL_NAME,
                        contents=full_prompt,
                        config=config,
                    )
                    logger.info(f"‚úÖ Reintento exitoso en intento {attempt}")
                    return response.text
                except APIError as retry_error:
                    if attempt == max_retries:
                        logger.error(f"\n{'='*60}")
                        logger.error("‚ùå TODOS LOS REINTENTOS FALLARON")
                        logger.error(f"{'='*60}")
                        logger.error(f"El servicio de Gemini sigue no disponible despu√©s de {max_retries} intentos")
                        logger.error(f"√öltima error: {retry_error}")
                        logger.error(f"\nüí° RECOMENDACIONES:")
                        logger.error(f"   1. Espera 5-10 minutos e intenta de nuevo")
                        logger.error(f"   2. Verifica el estado de Google AI: https://status.cloud.google.com/")
                        logger.error(f"   3. Considera usar otro modelo si est√° disponible")
                        logger.error(f"{'='*60}\n")
                        raise SystemExit(f"PROCESO CANCELADO: Servicio de Gemini no disponible despu√©s de {max_retries} reintentos.")
                    else:
                        logger.warning(f"   ‚ùå Intento {attempt} fall√≥: {retry_error}")
                        continue
        
        # Otros errores de API
        return f"ERROR_API: No se pudo conectar con Gemini. {e}"
        
    except Exception as e:
        return f"ERROR_GENERAL: {e}"
