"""
Cliente LLM para interacci√≥n con Google Gemini.
Incluye manejo de errores y reintentos autom√°ticos.
"""

import os
import time
from typing import Optional
from pydantic import BaseModel
from google import genai
from google.genai.errors import APIError
from config.settings import settings
from tools.code_executor import CodeExecutionToolWithInterpreterPY, CodeExecutionToolWithInterpreterTS

# Inicializaci√≥n del cliente Gemini
# try:
#     # Intentar usar userdata de Colab
#     from google.colab import userdata
#     os.environ["GEMINI_API_KEY"] = userdata.get('gen-lang-client-0440601098')
#     client = genai.Client()
#     print("‚úÖ Cliente Gemini inicializado correctamente (Colab).")
# except ImportError:
    # Entorno local - usar .env
if settings.GEMINI_API_KEY:
    client = genai.Client(api_key=settings.GEMINI_API_KEY)
    print("‚úÖ Cliente Gemini inicializado correctamente (Local).")
else:
    print("‚ö†Ô∏è WARNING: GEMINI_API_KEY no configurada. El cliente puede fallar.")
    client = None
# except Exception as e:
#     print(f"‚ùå ERROR: Fallo al inicializar el cliente Gemini. {e}")
#     client = None


def call_gemini(
    role_prompt: str, 
    context: str, 
    response_schema: Optional[BaseModel] = None, 
    allow_use_tool: bool = False
) -> str:
    """
    Realiza una llamada a Gemini 2.5 Flash con el prompt de rol y el contexto.
    
    Args:
        role_prompt (str): El prompt que define el rol y las instrucciones del agente
        context (str): El contexto actual del proyecto
        response_schema (BaseModel, optional): Schema Pydantic para validaci√≥n de respuesta JSON
        allow_use_tool (bool): Si se permite el uso de herramientas (tools)
    
    Returns:
        str: La respuesta del modelo LLM
    """
    if not client:
        return "ERROR: Cliente Gemini no inicializado correctamente."

    full_prompt = (
        f"{role_prompt}\n\n"
        f"--- DATOS ACTUALES DEL PROYECTO ---\n"
        f"{context}\n\n"
        f"--- TAREA ---\n"
    )

    config = {
        "temperature": settings.TEMPERATURE,
        "max_output_tokens": settings.MAX_OUTPUT_TOKENS
    }

    if response_schema:
        # Rama del Product Owner - salida JSON estructurada
        config["response_mime_type"] = "application/json"
        config["response_schema"] = response_schema.model_json_schema()
        full_prompt += (
            f"GENERA EL OUTPUT √öNICAMENTE EN FORMATO JSON que se adhiera al siguiente "
            f"esquema Pydantic: {response_schema.__name__}. "
            f"No a√±adas explicaciones ni texto adicional."
        )
    elif allow_use_tool:
        
        # Proveer ambas herramientas para que el modelo elija seg√∫n el lenguaje
        available_tools = [CodeExecutionToolWithInterpreterPY, CodeExecutionToolWithInterpreterTS]
        config["tools"] = available_tools
        full_prompt += "Genera √∫nicamente el bloque de texto solicitado en tu Output Esperado. No a√±adas explicaciones."
    else:
        full_prompt += "Genera √∫nicamente el bloque de texto solicitado en tu Output Esperado. No a√±adas explicaciones."

    try:
        response = client.models.generate_content(
            model=settings.MODEL_NAME,
            contents=full_prompt,
            config=config,
        )
        if not response.text or response.text == "None" or response.text.lower() == "none":
            print(f"\n{'='*60}")
            print("‚ùå ERROR: EL LLM NO DEVOLVI√ì RESPUESTA V√ÅLIDA")
            print(f"{'='*60}")
            print(f"üìã Informaci√≥n de diagn√≥stico:")
            print(f"   ‚Ä¢ Modelo usado: {settings.MODEL_NAME}")
            print(f"   ‚Ä¢ Respuesta vac√≠a: {response.text is None or response.text == ''}")
            print(f"   ‚Ä¢ Valor de response.text: {repr(response.text)}")
            print(f"   ‚Ä¢ Tipo de response: {type(response)}")
            
            # Verificar si hay candidatos en la respuesta
            if hasattr(response, 'candidates') and response.candidates:
                print(f"   ‚Ä¢ Candidatos disponibles: {len(response.candidates)}")
                for i, candidate in enumerate(response.candidates):
                    print(f"   ‚Ä¢ Candidato {i+1}:")
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason = str(candidate.finish_reason)
                        print(f"     - Finish reason: {finish_reason}")
                        
                        # Diagn√≥stico espec√≠fico para MALFORMED_FUNCTION_CALL
                        if "MALFORMED_FUNCTION_CALL" in finish_reason:
                            print(f"\n{'='*60}")
                            print("üîß DIAGN√ìSTICO: MALFORMED_FUNCTION_CALL")
                            print(f"{'='*60}")
                            print(f"El modelo intent√≥ llamar a una herramienta pero la llamada est√° mal formada.")
                            print(f"\nüìä Detalles del candidato:")
                            
                            # Mostrar contenido completo del candidato
                            if hasattr(candidate, 'content') and candidate.content:
                                print(f"   ‚Ä¢ Contenido del candidato:")
                                print(f"     {candidate.content}")
                                
                                # Verificar si hay function_calls
                                if hasattr(candidate.content, 'parts'):
                                    print(f"\n   ‚Ä¢ Partes del contenido ({len(candidate.content.parts)} partes):")
                                    for j, part in enumerate(candidate.content.parts):
                                        print(f"     - Parte {j+1}: {type(part).__name__}")
                                        if hasattr(part, 'function_call'):
                                            print(f"       ‚Üí Function call detectada:")
                                            print(f"         Nombre: {part.function_call.name if hasattr(part.function_call, 'name') else 'N/A'}")
                                            print(f"         Argumentos: {part.function_call.args if hasattr(part.function_call, 'args') else 'N/A'}")
                                        elif hasattr(part, 'text'):
                                            print(f"       ‚Üí Texto: {part.text[:200]}...")
                            
                            # Mostrar herramientas disponibles
                            if allow_use_tool:
                                print(f"\n   ‚Ä¢ Herramientas configuradas:")
                                if 'tools' in config:
                                    for tool in config['tools']:
                                        tool_name = tool.__name__ if hasattr(tool, '__name__') else str(tool)
                                        print(f"     - {tool_name}")
                            
                            print(f"\nüí° Posibles causas:")
                            print(f"   1. El modelo gener√≥ argumentos con formato JSON inv√°lido")
                            print(f"   2. Los argumentos no coinciden con el schema de la herramienta")
                            print(f"   3. Falta alg√∫n argumento requerido por la herramienta")
                            print(f"   4. El nombre de la funci√≥n es incorrecto")
                            print(f"{'='*60}\n")
                    
                    if hasattr(candidate, 'safety_ratings'):
                        print(f"     - Safety ratings: {candidate.safety_ratings}")
                    if hasattr(candidate, 'content'):
                        print(f"     - Content disponible: {candidate.content is not None}")
            else:
                print(f"   ‚Ä¢ No hay candidatos en la respuesta")
            
            # Verificar bloqueos de seguridad
            if hasattr(response, 'prompt_feedback'):
                print(f"   ‚Ä¢ Prompt feedback: {response.prompt_feedback}")
            
            print(f"{'='*60}\n")
            raise APIError("El LLM devolvi√≥ None o respuesta vac√≠a.")
        return response.text

    except APIError as e:
        # Detectar errores 503 (Service Unavailable) o sobrecarga
        error_message = str(e)
        
        if "503" in error_message or "UNAVAILABLE" in error_message or "overloaded" in error_message.lower():
            print(f"\n{'='*60}")
            print("‚ö†Ô∏è ERROR 503: SERVICIO SOBRECARGADO")
            print(f"{'='*60}")
            print(f"‚ùå El modelo de Gemini est√° sobrecargado")
            print(f"üìä Detalles: {e}")
            print(f"\nüîÑ REINTENTANDO con espera exponencial...")
            print(f"{'='*60}\n")
            
            # Reintentar con backoff exponencial
            max_retries = settings.MAX_API_RETRIES
            for attempt in range(1, max_retries + 1):
                wait_time = settings.RETRY_BASE_DELAY ** attempt  # 2, 4, 8 segundos
                print(f"üîÑ Intento {attempt}/{max_retries} - Esperando {wait_time}s...")
                time.sleep(wait_time)
                
                try:
                    response = client.models.generate_content(
                        model=settings.MODEL_NAME,
                        contents=full_prompt,
                        config=config,
                    )
                    print(f"‚úÖ Reintento exitoso en intento {attempt}")
                    return response.text
                except APIError as retry_error:
                    if attempt == max_retries:
                        print(f"\n{'='*60}")
                        print("‚ùå TODOS LOS REINTENTOS FALLARON")
                        print(f"{'='*60}")
                        print(f"El servicio de Gemini sigue no disponible despu√©s de {max_retries} intentos")
                        print(f"√öltima error: {retry_error}")
                        print(f"\nüí° RECOMENDACIONES:")
                        print(f"   1. Espera 5-10 minutos e intenta de nuevo")
                        print(f"   2. Verifica el estado de Google AI: https://status.cloud.google.com/")
                        print(f"   3. Considera usar otro modelo si est√° disponible")
                        print(f"{'='*60}\n")
                        raise SystemExit(f"PROCESO CANCELADO: Servicio de Gemini no disponible despu√©s de {max_retries} reintentos.")
                    else:
                        print(f"   ‚ùå Intento {attempt} fall√≥: {retry_error}")
                        continue
        
        # Otros errores de API
        return f"ERROR_API: No se pudo conectar con Gemini. {e}"
        
    except Exception as e:
        return f"ERROR_GENERAL: {e}"
