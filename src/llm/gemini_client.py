"""
Cliente LLM para interacci√≥n con Google Gemini.
Incluye manejo de errores y reintentos autom√°ticos.
Soporta wrapper de LangChain opcional para debugging avanzado.
"""

import os
import time
from typing import Optional, Any, Union, List, Dict
from pydantic import BaseModel
from google import genai
from google.genai.errors import APIError
from config.settings import settings
from utils.logger import setup_logger
from utils.logging_helpers import log_section
from llm.mock_responses import get_mock_response

logger = setup_logger(__name__, level=settings.get_log_level())


def _list_available_models() -> list[str]:
    """
    Lista los modelos disponibles en la API de Gemini.
    
    Returns:
        list[str]: Lista de nombres de modelos disponibles
    """
    try:
        if not client:
            return []
        
        models = client.models.list()
        available_models = []
        
        for model in models:
            # Filtrar solo modelos que soporten generateContent
            if hasattr(model, 'supported_generation_methods'):
                if 'generateContent' in model.supported_generation_methods:
                    available_models.append(model.name)
            else:
                # Si no tiene el atributo, incluirlo por defecto
                available_models.append(model.name)
        
        return available_models
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è No se pudo listar modelos disponibles: {e}")
        return []


def _safe_get_text(response: Any) -> str:
    """
    Extrae texto de forma segura de cualquier tipo de respuesta (objeto Response, str, dict, list).
    Garantiza compatibilidad hacia atr√°s y con nuevas versiones de API (gemini-3).
    
    Args:
        response: Respuesta del LLM en cualquier formato
        
    Returns:
        str: Texto extra√≠do o string vac√≠o si no se puede extraer
    """
    try:
        if response is None:
            return ""
            
        # 1. Si ya es string
        if isinstance(response, str):
            return response
            
        # 2. Si es lista (caso gemini-3 content list o LangChain messages)
        if isinstance(response, list):
            logger.debug(f"‚ÑπÔ∏è Respuesta es lista, uniendo elementos: {len(response)}")
            parts = []
            for item in response:
                if isinstance(item, str):
                    parts.append(item)
                elif isinstance(item, dict):
                    # Caso Gemini 3: {'type': 'text', 'text': '...'}
                    if item.get('type') == 'text' and 'text' in item:
                        parts.append(str(item['text']))
                    elif 'text' in item:
                        parts.append(str(item['text']))
                    else:
                        parts.append(str(item))
                elif hasattr(item, 'text'):
                    parts.append(item.text or "")
                else:
                    parts.append(str(item))
            return "\n".join(parts)
            
        # 3. Si es diccionario
        if isinstance(response, dict):
            # Caso espec√≠fico Gemini 3: {'type': 'text', 'text': '...'}
            if response.get('type') == 'text' and 'text' in response:
                logger.debug(f"‚ÑπÔ∏è Detectado formato Gemini 3: {{'type': 'text', 'text': '...'}}")
                return str(response['text'])
            
            # Prioridad de claves comunes en APIs de LLM
            for key in ['text', 'content', 'output', 'response', 'code']:
                if key in response:
                    val = response[key]
                    return _safe_get_text(val)
            # Si es un dict desconocido, convertir a str
            return str(response)

        # 4. Objeto Response de Google GenAI (prioridad a .text)
        if hasattr(response, 'text'):
            try:
                # En algunas versiones .text puede lanzar error si fue bloqueado
                text = response.text
                if text:
                    return text
            except Exception:
                pass # Intentar otras formas

        # 5. Intentar extraer de candidates (estructura interna de Gemini)
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            # Caso standard: content.parts
            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                parts = [part.text for part in candidate.content.parts if hasattr(part, 'text') and part.text]
                if parts:
                    return "\n".join(parts)
            # Caso fallback: content directo
            if hasattr(candidate, 'content') and isinstance(candidate.content, str):
                return candidate.content

        # 6. Objeto LangChain AIMessage
        if hasattr(response, 'content'):
            return _safe_get_text(response.content)

        # Fallback final: representaci√≥n string del objeto
        return str(response)
        
    except Exception as e:
        logger.error(f"‚ùå Error extrayendo texto de respuesta: {e}")
        return str(response)


# Importaci√≥n condicional del wrapper de LangChain
_langchain_available = False
if settings.USE_LANGCHAIN_WRAPPER:
    try:
        from llm.langchain_gemini import call_gemini_with_langchain, get_token_count
        _langchain_available = True
        logger.info("‚úÖ Wrapper de LangChain habilitado")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è No se pudo importar wrapper de LangChain: {e}")
        logger.warning("   Instala: pip install langchain-google-genai")
        _langchain_available = False

# Inicializaci√≥n del cliente Gemini
if settings.LLM_MOCK_MODE:
    client = None
    logger.info("üß™ LLM_MOCK_MODE=true: saltando inicializaci√≥n del cliente Gemini")
elif settings.GEMINI_API_KEY:
    client = genai.Client(api_key=settings.GEMINI_API_KEY)
    logger.info("‚úÖ Cliente Gemini inicializado correctamente.")
else:
    logger.warning("‚ö†Ô∏è WARNING: GEMINI_API_KEY no configurada. El cliente puede fallar.")
    client = None


def _log_warning_if_truncated(response, max_output_tokens: int) -> None:
    try:
        candidates = getattr(response, "candidates", None)
        if not candidates:
            return

        finish_reason = getattr(candidates[0], "finish_reason", None)
        finish_reason_str = str(finish_reason) if finish_reason is not None else ""
        fr = finish_reason_str.lower()

        if fr in {"max_tokens", "length", "token_limit"} or ("max" in fr and "token" in fr):
            logger.warning(
                "‚ö†Ô∏è Respuesta del LLM posiblemente TRUNCADA por l√≠mite de tokens "
                f"(finish_reason={finish_reason_str}, max_output_tokens={max_output_tokens}). "
                "Considera aumentar MAX_OUTPUT_TOKENS o pedir una salida m√°s corta."
            )
    except Exception:
        return


def call_gemini(
    role_prompt: str, 
    context: str = "", 
    response_schema: Optional[BaseModel] = None, 
    allow_use_tool: bool = False
) -> str:
    """
    Realiza una llamada a Gemini 2.5 Flash con el prompt formateado.
    
    Args:
        role_prompt (str): El prompt completo (puede incluir system + human de ChatPromptTemplate)
        context (str, optional): Contexto adicional (DEPRECATED - usar ChatPromptTemplate)
        response_schema (BaseModel, optional): Schema Pydantic para validaci√≥n de respuesta JSON
        allow_use_tool (bool): Si se permite el uso de herramientas (tools)
    
    Returns:
        str: La respuesta del modelo LLM
        
    Note:
        Con ChatPromptTemplate, el par√°metro 'context' ya no es necesario porque
        todo el prompt se construye en el template. Se mantiene por compatibilidad.
    """
    # MODO MOCK - Evitar llamadas reales al LLM durante testing
    if settings.LLM_MOCK_MODE:
        logger.info("üß™ [MOCK] Devolviendo respuesta mockeada (LLM_MOCK_MODE=true)")
        return get_mock_response(role_prompt, context)
    
    # MODO LANGCHAIN - Usar wrapper de LangChain si est√° habilitado
    # Nota: Solo para llamadas simples sin response_schema ni tools
    if settings.USE_LANGCHAIN_WRAPPER and _langchain_available:
        if response_schema is None and not allow_use_tool:
            logger.debug("üîó Usando wrapper de LangChain")
            try:
                return call_gemini_with_langchain(role_prompt, context)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error con wrapper LangChain, fallback a cliente directo: {e}")
                # Continuar con el cliente directo si falla
    
    if not client:
        return "ERROR: Cliente Gemini no inicializado correctamente."

    # Con ChatPromptTemplate, role_prompt ya contiene todo el prompt formateado
    # Solo a√±adir context si se proporciona (para compatibilidad con c√≥digo antiguo)
    if context:
        full_prompt = (
            f"{role_prompt}\n\n"
            f"--- DATOS ACTUALES DEL PROYECTO ---\n"
            f"{context}\n\n"
            f"--- TAREA ---\n"
        )
    else:
        # Prompt ya est√° completo desde ChatPromptTemplate
        full_prompt = role_prompt

    config = {
        "temperature": settings.TEMPERATURE,
        "max_output_tokens": settings.MAX_OUTPUT_TOKENS
    }

    if response_schema:
        # Rama del Product Owner - salida JSON estructurada
        config["response_mime_type"] = "application/json"
        config["response_schema"] = response_schema.model_json_schema()
        full_prompt += (
            f"GENERA EL OUTPUT √öNICAMENTE EN FORMATO JSON que se adhiera al siguiente "
            f"esquema Pydantic: {response_schema.__name__}. "
            f"No a√±adas explicaciones ni texto adicional."
        )
    else:
        full_prompt += "Genera √∫nicamente el bloque de texto solicitado en tu Output Esperado. No a√±adas explicaciones."

    try:
        response = client.models.generate_content(
            model=settings.MODEL_NAME,
            contents=full_prompt,
            config=config,
        )
        _log_warning_if_truncated(response, config.get("max_output_tokens", settings.MAX_OUTPUT_TOKENS))
        
        # Extraer texto de forma segura usando la nueva funci√≥n compatible con Gemini 3
        text_response = _safe_get_text(response)
        
        if not text_response or text_response == "None" or text_response.lower() == "none":
            logger.error("")
            log_section(logger, "‚ùå ERROR: EL LLM NO DEVOLVI√ì RESPUESTA V√ÅLIDA", level="error")
            logger.error(f"üìã Informaci√≥n de diagn√≥stico:")
            logger.error(f"   ‚Ä¢ Modelo usado: {settings.MODEL_NAME}")
            logger.error(f"   ‚Ä¢ Respuesta vac√≠a: {not text_response}")
            logger.error(f"   ‚Ä¢ Valor extra√≠do: {repr(text_response)}")
            logger.error(f"   ‚Ä¢ Tipo de response original: {type(response)}")
            
            # Verificar si hay candidatos en la respuesta
            if hasattr(response, 'candidates') and response.candidates:
                logger.error(f"   ‚Ä¢ Candidatos disponibles: {len(response.candidates)}")
                for i, candidate in enumerate(response.candidates):
                    logger.error(f"   ‚Ä¢ Candidato {i+1}:")
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason = str(candidate.finish_reason)
                        logger.error(f"     - Finish reason: {finish_reason}")
                        
                        # Diagn√≥stico espec√≠fico para MALFORMED_FUNCTION_CALL
                        if "MALFORMED_FUNCTION_CALL" in finish_reason:
                            logger.error("")
                            log_section(logger, "üîß DIAGN√ìSTICO: MALFORMED_FUNCTION_CALL", level="error")
                            logger.error(f"El modelo intent√≥ llamar a una herramienta pero la llamada est√° mal formada.")
                            logger.error(f"\nüìä Detalles del candidato:")
                            
                            # Mostrar contenido completo del candidato
                            if hasattr(candidate, 'content') and candidate.content:
                                logger.error(f"   ‚Ä¢ Contenido del candidato:")
                                logger.error(f"     {candidate.content}")
                                
                                # Verificar si hay function_calls
                                if hasattr(candidate.content, 'parts'):
                                    logger.error(f"\n   ‚Ä¢ Partes del contenido ({len(candidate.content.parts)} partes):")
                                    for j, part in enumerate(candidate.content.parts):
                                        logger.error(f"     - Parte {j+1}: {type(part).__name__}")
                                        if hasattr(part, 'function_call'):
                                            logger.error(f"       ‚Üí Function call detectada:")
                                            logger.error(f"         Nombre: {part.function_call.name if hasattr(part.function_call, 'name') else 'N/A'}")
                                            logger.error(f"         Argumentos: {part.function_call.args if hasattr(part.function_call, 'args') else 'N/A'}")
                                        elif hasattr(part, 'text'):
                                            logger.error(f"       ‚Üí Texto: {part.text[:200]}...")
                            
                            # Mostrar herramientas disponibles
                            if allow_use_tool:
                                logger.error(f"\n   ‚Ä¢ Herramientas configuradas:")
                                if 'tools' in config:
                                    for tool in config['tools']:
                                        tool_name = tool.__name__ if hasattr(tool, '__name__') else str(tool)
                                        logger.error(f"     - {tool_name}")
                            
                            logger.error(f"\nüí° Posibles causas:")
                            logger.error(f"   1. El modelo gener√≥ argumentos con formato JSON inv√°lido")
                            logger.error(f"   2. Los argumentos no coinciden con el schema de la herramienta")
                            logger.error(f"   3. Falta alg√∫n argumento requerido por la herramienta")
                            logger.error(f"   4. El nombre de la funci√≥n es incorrecto")
                    
                    if hasattr(candidate, 'safety_ratings'):
                        logger.error(f"     - Safety ratings: {candidate.safety_ratings}")
                    if hasattr(candidate, 'content'):
                        logger.error(f"     - Content disponible: {candidate.content is not None}")
            else:
                logger.error(f"   ‚Ä¢ No hay candidatos en la respuesta")
            
            # Verificar bloqueos de seguridad
            if hasattr(response, 'prompt_feedback'):
                logger.error(f"   ‚Ä¢ Prompt feedback: {response.prompt_feedback}")
            logger.error("")
            raise APIError("El LLM devolvi√≥ None o respuesta vac√≠a.")
        return text_response
    
    except APIError as e:
        # Detectar errores cr√≠ticos que deben detener el flujo
        error_message = str(e)
        
        # Error 404: Modelo no encontrado - DETENER FLUJO
        if "404" in error_message or "NOT_FOUND" in error_message or "is not found" in error_message.lower():
            logger.error("")
            log_section(logger, "‚ùå ERROR 404: MODELO NO ENCONTRADO", level="error")
            logger.error(f"‚ùå El modelo especificado no existe o no est√° disponible")
            logger.error(f"üìä Detalles: {e}")
            logger.error(f"ÔøΩ Modelo solicitado: {settings.MODEL_NAME}")
            
            # Intentar listar modelos disponibles
            logger.error(f"\nüîç Consultando modelos disponibles en tu API key...")
            available_models = _list_available_models()
            
            if available_models:
                logger.error(f"\n‚úÖ Modelos disponibles con generateContent:")
                for i, model in enumerate(available_models, 1):
                    logger.error(f"   {i}. {model}")
            else:
                logger.error(f"\n‚ö†Ô∏è No se pudo obtener la lista de modelos disponibles")
                logger.error(f"   Modelos comunes: gemini-2.0-flash-exp, gemini-1.5-flash, gemini-1.5-pro")
            
            logger.error(f"\nüí° RECOMENDACIONES:")
            logger.error(f"   1. Verifica el nombre del modelo en .env (MODEL_NAME)")
            logger.error(f"   2. Usa uno de los modelos listados arriba")
            logger.error(f"   3. Consulta la documentaci√≥n: https://ai.google.dev/gemini-api/docs/models")
            logger.error(f"   4. Verifica que tu API key tenga acceso al modelo")
            logger.error("")
            raise RuntimeError(f"ERROR_404_MODEL_NOT_FOUND: {e}")
        
        # Detectar errores 503 (Service Unavailable) o sobrecarga
        if "503" in error_message or "UNAVAILABLE" in error_message or "overloaded" in error_message.lower():
            logger.error("")
            log_section(logger, "‚ö†Ô∏è ERROR 503: SERVICIO SOBRECARGADO", level="error")
            logger.error(f"‚ùå El modelo de Gemini est√° sobrecargado")
            logger.error(f"üìä Detalles: {e}")
            logger.error(f"\nüîÑ REINTENTANDO con espera exponencial...")
            logger.error("")
            
            # Reintentar con backoff exponencial
            max_retries = settings.MAX_API_RETRIES
            for attempt in range(1, max_retries + 1):
                wait_time = settings.RETRY_BASE_DELAY ** attempt  # 2, 4, 8 segundos
                logger.warning(f"üîÑ Intento {attempt}/{max_retries} - Esperando {wait_time}s...")
                time.sleep(wait_time)
                
                try:
                    response = client.models.generate_content(
                        model=settings.MODEL_NAME,
                        contents=full_prompt,
                        config=config,
                    )
                    _log_warning_if_truncated(response, config.get("max_output_tokens", settings.MAX_OUTPUT_TOKENS))
                    logger.info(f"‚úÖ Reintento exitoso en intento {attempt}")
                    # Usar _safe_get_text tambi√©n en reintentos para compatibilidad con Gemini 3
                    return _safe_get_text(response)
                except APIError as retry_error:
                    if attempt == max_retries:
                        logger.error("")
                        log_section(logger, "‚ùå TODOS LOS REINTENTOS FALLARON", level="error")
                        logger.error(f"El servicio de Gemini sigue no disponible despu√©s de {max_retries} intentos")
                        logger.error(f"√öltima error: {retry_error}")
                        logger.error(f"\nüí° RECOMENDACIONES:")
                        logger.error(f"   1. Espera 5-10 minutos e intenta de nuevo")
                        logger.error(f"   2. Verifica el estado de Google AI: https://status.cloud.google.com/")
                        logger.error(f"   3. Considera usar otro modelo si est√° disponible")
                        logger.error(f"   4. Activa LLM_MOCK_MODE=true en .env para testing sin API")
                        logger.error("")
                        
                        # Retornar error estructurado en lugar de SystemExit
                        return f"ERROR_503_MAX_RETRIES: Servicio no disponible despu√©s de {max_retries} intentos. {retry_error}"
                    else:
                        logger.warning(f"   ‚ùå Intento {attempt} fall√≥: {retry_error}")
                        continue
        
        # Otros errores de API
        return f"ERROR_API: No se pudo conectar con Gemini. {e}"
        
    except Exception as e:
        return f"ERROR_GENERAL: {e}"
